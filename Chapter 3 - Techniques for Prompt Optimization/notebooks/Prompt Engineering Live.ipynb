{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Prompt Engineering\n",
    "by Karen (inspired by https://github.com/dair-ai/Prompt-Engineering-Guide)\n",
    "\n",
    "\n",
    "This notebook contains examples and exercises to learning about prompt engineering.\n",
    "\n",
    "API key from PlusGPT was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"PLUSGPT_API_KEY\"] = os.getenv(\"PLUSGPT_API_KEY\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt Engineering Basics\n",
    "\n",
    "Objectives\n",
    "- Load the libraries\n",
    "- Review the format\n",
    "- Cover basic prompts\n",
    "- Review common use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are loading the necessary libraries, utilities, and configurations. \n",
    "\n",
    "Please install `dotenv` as follows: `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(data):\n",
    "    \"\"\" GET completion response from plusgpt api\"\"\"\n",
    "    response = requests.post(\"https://api.plusgptcloud.com\", headers=headers, json=data, timeout=70)\n",
    "    print(response.json()['choices'][0]['message']['content'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_prompt():\n",
    "  while True:\n",
    "    user_input = input()\n",
    "    data = {\n",
    "  \"user\": f\"\"\"\n",
    "system: principal data engineer\n",
    "prompt: {user_input}\n",
    "  \"\"\",\n",
    "  \"lang\": \"english\"\n",
    "}\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    get_response(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Kafka is\n",
      "\n",
      "\n",
      "Apache Kafka is an open-source distributed event streaming platform that is gaining widespread adoption in modern architecture. It provides a fast, scalable, and fault-tolerant messaging system to efficiently process and integrate large volumes of data from various sources in real-time. Apache Kafka supports a range of use cases such as streaming data processing, event driven architecture, and microservices communication. It offers many benefits such as low latency, high throughput, and data integration. As a principal data engineer, one can leverage the power of Apache Kafka to build resilient and scalable data pipelines for real-time data processing and analytics.\n",
      "link me to the official document describing Kafka\n",
      "\n",
      "\n",
      "I'm sorry, but I cannot browse the internet or search for official documents as my capabilities are limited to generating human-like text based on the input given to me.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# basic example\n",
    "prompt=\"Apache Kafka is\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What is event streaming? Event streaming is the digital equivalent of the human body's central nervous system. It is the technological foundation for the 'always-on' world where businesses are increasingly software-defined and automated, and where the user of software is more software.  Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time.  What can I use event streaming for? Event streaming is applied to a wide variety of use cases across a plethora of industries and organizations. Its many examples include:  To process payments and financial transactions in real-time, such as in stock exchanges, banks, and insurances. To track and monitor cars, trucks, fleets, and shipments in real-time, such as in logistics and the automotive industry. To continuously capture and analyze sensor data from IoT devices or other equipment, such as in factories and wind parks. To collect and immediately react to customer interactions and orders, such as in retail, the hotel and travel industry, and mobile applications. To monitor patients in hospital care and predict changes in condition to ensure timely treatment in emergencies. To connect, store, and make available data produced by different divisions of a company. To serve as the foundation for data platforms, event-driven architectures, and microservices.\" summarize these paragraphs for me.\n",
      "\n",
      "\n",
      "Event streaming is the practice of capturing and processing data in real-time from various sources like databases, sensors, and software applications. It allows for the continuous flow and interpretation of data, making sure the right information is available at the right place and time. Many industries and organizations use event streaming for purposes such as processing payments, monitoring vehicles and shipments, analyzing sensor data, and collecting customer interactions and orders. It can also be used as the foundation for data platforms and microservices.\n",
      "summarize the above paragraph to me like I am 5\n",
      "\n",
      "\n",
      "A principal data engineer is someone who helps organize and make sense of a lot of information to help make decisions. They are really good at working with computers and technology.\n",
      "explain what you answered about event streaming to me like I am 5\n",
      "\n",
      "\n",
      "Event streaming is like watching a river flow. Just like how you can watch the water move in a river, event streaming lets us see the movement of data from different places. It helps us keep track of what is happening in different parts of a system and make decisions based on that information.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. Keep the answer short and concise. Respond 'Unsure about answer' if not sure about the answer.  Context: Kafka topics are separated into partitions, each of which contains records in a fixed order. A unique offset is assigned and attributed to each record in a partition. Multiple partition logs can be found in a single topic. This allows several users to read from the same topic at the same time. Topics can be parallelized via partitions, which split data into a single topic among numerous brokers.  Replication in Kafka is done at the partition level. A replica is the redundant element of a topic partition. Each partition often contains one or more replicas, which means that partitions contain messages that are duplicated across many Kafka brokers in the cluster.  One server serves as the leader of each partition (replica), while the others function as followers. The leader replica is in charge of all read-write requests for the partition, while the followers replicate the leader. If the lead server goes down, one of the followers takes over as the leader. To disperse the burden, we should aim for a good balance of leaders, with each broker leading an equal number of partitions.  Question: What is a partition in Kafka?\n",
      "A partition in Kafka is a way to separate topics into smaller, ordered logs which contain records with unique offsets assigned to each one. Multiple partitions can be found in a single topic, allowing for parallelization and redundancy through replicated leader and follower servers.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=\"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond 'Unsure about answer' if not sure about the answer.\n",
    "\n",
    "Context: Kafka topics are separated into partitions, each of which contains records in a fixed order. A unique offset is assigned and attributed to each record in a partition. Multiple partition logs can be found in a single topic. This allows several users to read from the same topic at the same time. Topics can be parallelized via partitions, which split data into a single topic among numerous brokers.\n",
    "\n",
    "Replication in Kafka is done at the partition level. A replica is the redundant element of a topic partition. Each partition often contains one or more replicas, which means that partitions contain messages that are duplicated across many Kafka brokers in the cluster.\n",
    "\n",
    "One server serves as the leader of each partition (replica), while the others function as followers. The leader replica is in charge of all read-write requests for the partition, while the followers replicate the leader. If the lead server goes down, one of the followers takes over as the leader. To disperse the burden, we should aim for a good balance of leaders, with each broker leading an equal number of partitions.\n",
    "\n",
    "Question: What is a partition in Kafka?\"\"\"\n",
    "get_response_from_prompt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the text into neutral, negative or positive. explain why  Text: I think Apache Kafka is a great tech stack for data streaming\n",
      "\n",
      "\n",
      "Classification: Positive\n",
      "\n",
      "Explanation: The text contains positive sentiment as the author thinks that Apache Kafka is a great technology stack for data streaming. There is no indication of negativity or criticism in the statement. Therefore, it can be classified as positive.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=\"\"\"Classify the text into neutral, negative or positive. explain why\n",
    "\n",
    "Text: I think Apache Kafka is a great tech stack for data streaming\n",
    "\n",
    "\"\"\"\n",
    "get_response_from_prompt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: Principal data engineer   prompt: I am a junior data engineer eager to learn more about Apache Kafka. Tell me what are the core concepts of Kafka that I should know   goal: link me to the resources to these concepts as well as the sample code, so that I can know more \n",
      "\n",
      "\n",
      "As a junior data engineer, it's great to hear that you are eager to learn about Apache Kafka! Here are some core concepts that you should know:\n",
      "\n",
      "1. Topic: A category or feed name to which records are published.\n",
      "2. Producer: An application that sends messages to the Kafka cluster.\n",
      "3. Consumer: An application that reads messages from the Kafka cluster.\n",
      "4. Broker: A Kafka server that manages the storage and communication of messages.\n",
      "5. Partition: A unit of parallelism in Kafka that allows for distributed processing of messages.\n",
      "6. Offset: A unique identifier for each message within a partition.\n",
      "\n",
      "To get started with learning more about Apache Kafka, I recommend first checking out the official Apache Kafka documentation: https://kafka.apache.org/documentation/. There, you can learn about the core concepts as well as the API reference, setup and configuration, and more.\n",
      "\n",
      "Next, I suggest taking a look at the Apache Kafka GitHub repository for sample code and some useful tools: https://github.com/apache/kafka. You can also head to the Confluent website (https://www.confluent.io/) for additional resources and tutorials.\n",
      "\n",
      "Good luck on your journey to becoming a proficient data engineer with Apache Kafka!\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt=\"\"\"role: Principal data engineer\n",
    "  prompt: I am a junior data engineer eager to learn more about Apache Kafka. Tell me what are the core concepts of Kafka that I should know\n",
    "  goal: link me to the resources to these concepts as well as the sample code, so that I can know more \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: Principal data engineer   prompt: Provide me with sample code for an application using Apache Kafka\n",
      "\n",
      "\n",
      "As a Principal data engineer, I can provide you with a sample code for an application using Apache Kafka.\n",
      "\n",
      "First, we need to install the Kafka package using the following command:\n",
      "\n",
      "```\n",
      "pip install kafka-python\n",
      "```\n",
      "\n",
      "Once installed, we can create a producer application that sends data to a Kafka topic. Here's an example code:\n",
      "\n",
      "```python\n",
      "from kafka import KafkaProducer\n",
      "\n",
      "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
      "\n",
      "for i in range(10):\n",
      "    producer.send('test-topic', value='Hello from Kafka!'.encode())\n",
      "\n",
      "producer.flush()\n",
      "```\n",
      "\n",
      "Here, we import the `KafkaProducer` class and create a new instance, passing the `bootstrap_servers` parameter with the IP address of the Kafka broker.\n",
      "\n",
      "Next, we iterate through a range of ten messages and use `producer.send()` method to send each message to a Kafka topic named `'test-topic'`.\n",
      "\n",
      "Finally, we call `producer.flush()` to make sure all messages are successfully sent before exiting the program.\n",
      "\n",
      "This is just a basic example, but there are many advanced features and configurations that can be used with Kafka to build robust streaming applications.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "  role: Principal data engineer\n",
    "  prompt: Provide me with sample code for an application using Apache Kafka\n",
    "  \n",
    "  \"\"\"\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to provide mentorship to students that are interested in learning Apache Kafka, but did not have prior experience.    provide a 3-month study plan for someone that has time to study 10 hours/week. Break the study plan into weeks. First, list the concepts that needs to be understood, then, provide study materials with url links and hands on projects with sample repos.    The end goal is to have basic understanding, and hands-on experience with Kafka.\n",
      "\n",
      "\n",
      "Week 1-2: Introduction to Kafka\n",
      "- Understand the basic concepts of Kafka, such as topics, partitions, brokers and zookeepers\n",
      "- Install Kafka and setup a local cluster\n",
      "- Learn to use the command line interface to create topics and produce/consume messages\n",
      "- Explore Kafka documentation and community resources\n",
      "\n",
      "Study Materials:\n",
      "- Apache Kafka Documentation: https://kafka.apache.org/documentation/\n",
      "- Kafka Tutorial for Beginners: https://www.tutorialspoint.com/apache_kafka/apache_kafka_tutorial.pdf\n",
      "- Kafka Quickstart Guide: https://kafka.apache.org/quickstart\n",
      "\n",
      "Hand-on Project:\n",
      "- Setting up a local Kafka Cluster using Docker\n",
      "- Creating a simple Kafka Producer and Consumer application using Java or Python\n",
      "\n",
      "Week 3-4: Kafka Architecture\n",
      "- Explore Kafka’s distributed architecture and its components\n",
      "- Dive deeper on the components such as Producer, Consumer, Connect and Streams APIs\n",
      "- Learn about Kafka topics and partitioning strategies\n",
      "- Understand Kafka’s fault-tolerance and scalability features\n",
      "\n",
      "Study Materials:\n",
      "- Kafka Architecture Guide: https://kafka.apache.org/documentation/#intro_architecture\n",
      "- Understanding Kafka Consumer groups: https://www.cloudkarafka.com/blog/2018-07-04-understanding-kafka-consumer-groups.html\n",
      "- Apache Kafka Certification Course: https://www.udemy.com/course/apache-kafka/\n",
      "\n",
      "Hand-on Project:\n",
      "- Creating a Kafka Consumer and Producer with custom Partitioners \n",
      "- Implementing Kafka brokers fail-over and recovery \n",
      "\n",
      "Week 5-6: Kafka for Big Data and Real-time processing\n",
      "- Explore integration with Hadoop, Spark or other Big Data Platforms\n",
      "- Learn about Kafka Connect for building File, Database or API Connectors \n",
      "- Understand Kafka Streams for real-time processing of data streams \n",
      "- Understanding Kafka Producers for Microservices based architectures \n",
      "\n",
      "Study Materials: \n",
      "- Apache Kafka for Big Data: https://www.cloudera.com/products/open-source/apache-hadoop/apache-kafka.html\n",
      "- Kafka Connect Documentation: https://docs.confluent.io/platform/current/connect/index.html\n",
      "- Kafka Streams Introduction: https://kafka.apache.org/documentation/streams/\n",
      "\n",
      "Hand-on Project:\n",
      "- Build a simple real-time data processing pipeline with Kafka and Spark Streaming \n",
      "- Implement a custom Kafka Connectors for Salesforce \n",
      "\n",
      "Week 7-8: Monitoring, Optimization and Troubleshooting \n",
      "- Learn Kafka’s monitoring and diagnostic features \n",
      "- Understand the techniques of optimizing Kafka performance \n",
      "- Reviewing Kafka logs and troubleshooting common issues \n",
      "\n",
      "Study Materials:\n",
      "- Kafka monitoring and metrics: https://www.datadoghq.com/blog/configuring-kafka-monitoring-and-metrics/\n",
      "- Kafka performance Tuning: https://docs.confluent.io/platform/current/tutorials/cp-demo/docs/index.html#optimizing-for-throughput\n",
      "- Kafka troubleshooting: https://dzone.com/articles/15-kafka-troubleshooting-tips \n",
      "\n",
      "Hand-on Project:\n",
      "- Monitor Kafka cluster performance using dataDog \n",
      "- Configure and Test different Kafka configurations for throughput \n",
      "\n",
      "Week 9-10: Secure and Manage Kafka\n",
      "- Learn Kafka’s security features such as Authentication, Authorization and Encryption \n",
      "- Configure Kafka for Multi-tenancy and centralized management \n",
      "- Understanding Kafka integration with Kubernetes and cloud-managed services \n",
      "\n",
      "Study Materials:\n",
      "- Kafka Security: https://kafka.apache.org/documentation/#security\n",
      "- Kafka Admin documentation: https://kafka.apache.org/documentation/#adminapi \n",
      "- Kafka on Kubernetes: https://dzone.com/articles/apache-kafka-and-kubernetes\n",
      "\n",
      "Hand-on Project:\n",
      "- Securing the Kafka cluster using SSL and SASL \n",
      "- Automating Kafka Operations using Ansible \n",
      "\n",
      "By the end of this 3-month study plan, you will have a basic understanding of Apache Kafka, its features, and use cases, as well as hands-on experience building applications and solutions with Kafka.\n",
      "Can you link me to some sample repos for hands-on project\n",
      "\n",
      "\n",
      "Certainly! Here are some sample GitHub repositories for data engineering projects that might be useful for hands-on practice:\n",
      "\n",
      "1. Data Engineering Cookbook - a collection of examples and techniques for data engineering tasks such as loading, processing, and transforming data.\n",
      "\n",
      "https://github.com/andkret/Cookbook.git \n",
      "\n",
      "2. Kafka Tutorial - a step-by-step tutorial on setting up and using Apache Kafka for data streaming.\n",
      "\n",
      "https://github.com/confluentinc/kafka-tutorial.git \n",
      "\n",
      "3. Airflow Tutorial - a tutorial on how to use Apache Airflow for managing and scheduling data pipelines.\n",
      "\n",
      "https://github.com/hgrif/airflow-tutorial.git \n",
      "\n",
      "4. Streaming ETL Pipeline Example - an example of a complete end-to-end streaming ETL pipeline using Apache Kafka, Spark Streaming, and Apache Cassandra.\n",
      "\n",
      "https://github.com/aoperrin/streaming-etl-pipeline-example.git \n",
      "\n",
      "5. Real-Time Analytics with Spark Streaming - a tutorial on how to use Apache Spark for real-time analytics with a focus on Spark Streaming.\n",
      "\n",
      "https://github.com/databricks/Spark-The-Definitive-Guide.git \n",
      "\n",
      "I hope you find these resources helpful!\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  prompt=\"\"\"\n",
    "  \n",
    "  I want you to provide mentorship to students that are interested in learning Apache Kafka, but did not have prior experience.\n",
    "   provide a 3-month study plan for someone that has time to study 10 hours/week. Break the study plan into weeks. First, list the concepts that needs to be understood, then, provide study materials with url links and hands on projects with sample repos.\n",
    "   The end goal is to have basic understanding, and hands-on experience with Kafka.\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Prompting Techniques\n",
    "\n",
    "Objectives:\n",
    "\n",
    "- Cover more advanced techniques for prompting: few-shot, chain-of-thoughts,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Zero-shot Promps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "design a project to analyze a large dataset (ecommerce data) using Apache kafka.   Let's think step by step. provide related links and sample repo for each step\n",
      "\n",
      "\n",
      "Step 1: Define the problem statement and data requirements.\n",
      "The first step in analyzing an eCommerce dataset using Apache Kafka is to define the problem statement and identify the specific data that needs to be analyzed. This includes deciding on the data sources to be used, whether it’s customer behavior, payment information, or product information. In addition to identifying the data sources, you should also identify the data requirements such as data volume, velocity, and variety.\n",
      "\n",
      "Step 2: Collect the data and store it in the data lake.\n",
      "The next step is to collect the data through various data sources like data warehouses or API requests. Data is then stored in the data lake, which is used as the basis for analysis. In this step, we will transform the data to a format that can be consumed by Apache Kafka.\n",
      "\n",
      "Step 3: Configure Apache Kafka and set up data pipelines.\n",
      "We will configure Apache Kafka and create data pipelines to stream the data from the data lake to the Apache Kafka producer. The producer is responsible for writing data to the Kafka cluster. The pipelines should be set such that they dynamically adjust to changes in the data schema.\n",
      "\n",
      "Step 4: Create consumer applications to analyze data.\n",
      "The next step is to create the consumer applications that will analyze the data that is being produced by Apache Kafka. The consumer application can also be configured to consume data in real-time from the Kafka cluster. We will use Apache Spark to analyze the data using different Spark libraries that can handle structured, semi-structured, or unstructured data.\n",
      "\n",
      "Step 5: Visualize the results.\n",
      "After analyzing the data, we need to present the results in an easy-to-understand format. For this, we will use various visualization tools to create charts, dashboards, and graphs.\n",
      "\n",
      "Github repos:\n",
      "\n",
      "1) Python API Client for Apache Kafka – https://github.com/aiven/aiven-kafka-python\n",
      "\n",
      "2) Apache Kafka Producer and Consumers. https://github.com/cloudurable/kafka-samples/blob/master/kafka-producer-consumer-basics/src/main/java/io/cloudurable/kafka/producer/KafkaProducer.java\n",
      "\n",
      "3) Apache Spark Examples. https://github.com/apache/spark/tree/master/examples\n",
      "\n",
      "4) Visualization using Apache Zeppelin – https://zeppelin.apache.org/docs/0.7.0/interpreter/kafka.html\n",
      "\n",
      "References:\n",
      "\n",
      "1) Kafka Tutorial for Beginners – https://www.edureka.co/blog/kafka-tutorial/\n",
      "\n",
      "2) A Comprehensive Guide to Apache Kafka – https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-centos-7\n",
      "\n",
      "3) Best Python libraries for Apache Kafka – https://towardsdatascience.com/best-python-libraries-for-apache-kafka-79f594e570cc\n",
      "\n",
      "4) Apache Spark Introduction – https://spark.apache.org/docs/latest/index.html\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  prompt=\"\"\"design a project to analyze a large dataset (ecommerce data) using Apache kafka. \n",
    " Let's think step by step. provide related links and sample repo for each step\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Few-shot prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knowledge: Messages Retaining - comparison between Kafka and Traditional queuing system   Traditional queuing systems — Most queueing systems remove the messages after it has been processed typically from the end of the queue.  Apache Kafka — Here, messages persist even after being processed. They don’t get removed as consumers receive them.      knowledge: Logic-based processing   Traditional queuing systems — It does not allow to process logic based on similar messages or events.   Apache Kafka — It allows to process logic based on similar messages or events.   prompt: Your team is responsible for building a high-throughput messaging system for a social media website. Choose between Kafka and traditional queuing system and tell me why\n",
      "\n",
      "\n",
      "As the principal data engineer, I would recommend using Apache Kafka for building a high-throughput messaging system for a social media website. One reason to choose Kafka is that it retains messages even after they have been processed, which can be useful for auditing and other applications that require the ability to go back and analyze past events. Traditional queuing systems remove messages after they have been processed, which could limit the system's usefulness for these kinds of applications.\n",
      "\n",
      "Another reason to choose Kafka is its ability to process logic based on similar messages or events. This can be particularly useful in a social media context, where there may be many different types of events and interactions that need to be tracked and analyzed. Kafka's ability to group similar messages together allows for more efficient processing and analysis of this data.\n",
      "\n",
      "Overall, I believe that Apache Kafka would be the superior choice for building a high-throughput messaging system for a social media website due to its superior message retention and logic-based processing capabilities.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "  knowledge: Messages Retaining - comparison between Kafka and Traditional queuing system\n",
    "  Traditional queuing systems — Most queueing systems remove the messages after it has been processed typically from the end of the queue.\n",
    "\n",
    "Apache Kafka — Here, messages persist even after being processed. They don’t get removed as consumers receive them.\n",
    "  \n",
    "  knowledge: Logic-based processing\n",
    "  Traditional queuing systems — It does not allow to process logic based on similar messages or events.\n",
    "\n",
    " Apache Kafka — It allows to process logic based on similar messages or events.\n",
    "  prompt: Your team is responsible for building a high-throughput messaging system for a social media website. Choose between Kafka and traditional queuing system and tell me why\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Kafka is popular in e-commerce industry, explain why   A: In this era of e-commerce, winning over user attention is half the battle won, and the key is providing a relevant and personalised user experience. And with these same customers interacting digitally, organisations have more than enough data for personalisation. But it needs to happen in real-time—ideally during the interaction itself. Stream processing makes this possible by aggregating all related and relevant data and then creating a complete profile of the said customer.   Q: Kafka is popular for fraud detection, explain why   A: Industries collectively lose trillions of dollars in revenue annually due to fraud, and it is only getting worse. The good news is that stream processing can minimise cases of fraud by processing and analysing real-time streams of financial records, recognising patterns, uncovering suspicious transactions and creating predictive alerts for possible fraud.   Q; Kafka is popular for stock market monitoring, explain why   A: Players in the stock market are like all other customers: Discerning and demanding. This means they want, among other things, real-time reporting and faster SLA requirements, and expect transactional queries to be addressed immediately. There is, therefore, a need to analyse petabytes of data in real-time if these expectations are to be met. Stream processing can do this to deliver real-time data analytics and meet customer demands quickly.   prompt: Explain why Kafka has great business value for real time data processing\n",
      "\n",
      "\n",
      "Kafka has great business value for real-time data processing because it enables the processing and analysis of data in real-time, providing businesses with immediate insights and actionable information. In today's fast-paced business environment, where customers demand immediate responses and personalised experiences, real-time data processing has become essential. Kafka's distributed streaming platform enables businesses to process and analyse streaming data, allowing them to monitor and respond to events and trends in real-time. \n",
      "\n",
      "Real-time data processing can help businesses make informed decisions, improve customer experiences, detect fraud, and optimize operations. Kafka's ability to handle large volumes of data, scale horizontally, and provide fault-tolerance ensures that businesses can process and analyse data reliably and efficiently. This makes it an ideal solution for industries such as e-commerce, finance, and healthcare, where real-time data processing is critical. \n",
      "\n",
      "In addition to its technical capabilities, Kafka's open-source nature and strong community support make it a cost-effective solution for businesses of all sizes. It allows businesses to leverage real-time data processing without investing extensively in proprietary software solutions. This not only provides businesses with cost savings but also the flexibility to customize and adapt the platform to their specific needs. \n",
      "\n",
      "In conclusion, Kafka's ability to handle real-time data processing, scalability, fault-tolerance, and cost-effectiveness makes it an indispensable tool for modern businesses. By implementing Kafka, businesses can gain a competitive advantage by providing real-time personalised experiences, detecting fraud, and optimising operations to meet customer demands quickly.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "#https://datastorageasean.com/blogs/5-use-cases-stream-processing-demonstrate-its-business-value-0\n",
    "prompt=\"\"\"\n",
    "  knowledge: Q: Kafka is popular in e-commerce industry, explain why\n",
    "  A: In this era of e-commerce, winning over user attention is half the battle won, and the key is providing a relevant and personalised user experience. And with these same customers interacting digitally, organisations have more than enough data for personalisation. But it needs to happen in real-time—ideally during the interaction itself. Stream processing makes this possible by aggregating all related and relevant data and then creating a complete profile of the said customer.\n",
    "  Q: Kafka is popular for fraud detection, explain why\n",
    "  A: Industries collectively lose trillions of dollars in revenue annually due to fraud, and it is only getting worse. The good news is that stream processing can minimise cases of fraud by processing and analysing real-time streams of financial records, recognising patterns, uncovering suspicious transactions and creating predictive alerts for possible fraud.\n",
    "  Q; Kafka is popular for stock market monitoring, explain why\n",
    "  A: Players in the stock market are like all other customers: Discerning and demanding. This means they want, among other things, real-time reporting and faster SLA requirements, and expect transactional queries to be addressed immediately. There is, therefore, a need to analyse petabytes of data in real-time if these expectations are to be met. Stream processing can do this to deliver real-time data analytics and meet customer demands quickly.\n",
    "  prompt: Explain why Kafka has great business value for real time data processing\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Self-Consistency\n",
    "As an exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#self-consistency) and try them here. \n",
    "\n",
    "### 2.6 Generate Knowledge Prompting\n",
    "\n",
    "As an exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#generated-knowledge-prompting) and try them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: principal data engineer tutoring junior data engineers   knowledge: \\\"\\\"\\\"   Q: Why am I seeing Receive failed: Disconnected?   A: If the remote peer, typically the broker (but could also be an active TCP gateway of some kind), closes the connection you'll see a log message like this:  %3|1500588440.537|FAIL|rdkafka#producer-1| 10.255.84.150:9092/1: Receive failed: Disconnected  There are a number of possible reasons, in order of how common they are:  Broker's idle connection reaper closes the connection due to inactivity. This is controlled by the broker configuration property connections.max.idle.ms and defaults to 10 minutes. This is by far the most common reason for spontaneous disconnects. The client sent an unsupported protocol request; see Broker version compatibility. This is considered a configuration error on the client. The broker should log an exception explaining why the connection was closed, see the broker logs. The client sent a malformed protocol request; this is an indication of a bug in the client. The broker should log an exception explaining why the connection was closed, see the broker logs. The broker is in an invalid state. The broker should log an exception explaining why the connection was closed, see the broker logs. TCP gateway/load-balancer/firewall session timeout. Try enabling TCP keep-alives on the client by setting socket.keepalive.enable=true. Since a TCP close can't signal why the remote peer closed the connection there is no way for the client to know what went wrong. If the disconnect logs are getting annoying and the admin deems they are caused by the idle connection reaper, the log.connection.close client configuration property can be set to false to silence all spontaneous disconnect logs.   Q: Why am I not seeing any messages?   A: If there are no stored offsets for a partition (and group in case of the KafkaConsumer) the consumer will default its starting offset to the topic configuration setting auto.offset.reset which defaults to latest - that is, it will start consuming at the current end of a partition.  If you are using the KafkaConsumer you probably do not have a per-topic configuration object but should use the default topic config, see default_topic_conf.   \\\"\\\"\\\"   prompt: I am getting Receive failed: Disconnected error, I doubt if I sent an unsupported protocol request. tell me how to solve the issue\n",
      "\n",
      "\n",
      "If you are receiving a \"Receive failed: Disconnected\" error message, it may be due to a number of reasons. The most common reason is that the broker's idle connection reaper closes the connection due to inactivity. This can be solved by setting the broker configuration property connections.max.idle.ms to a higher value.\n",
      "\n",
      "Another possible reason for this error is that the client sent an unsupported protocol request. This is a configuration error on the client side and the broker should log an exception explaining why the connection was closed. Check the broker logs to see what the issue might be and adjust the client configuration as needed.\n",
      "\n",
      "If the client sent a malformed protocol request, it could be an indication of a bug in the client code. Again, check the broker logs to see why the connection was closed and address the issue accordingly.\n",
      "\n",
      "In some cases, the broker may be in an invalid state, causing the connection to be closed. Check the broker logs to see if this is the case and take corrective action as needed.\n",
      "\n",
      "Finally, there may be an issue with the TCP gateway/load-balancer/firewall session timeout. Enabling TCP keep-alives on the client by setting socket.keepalive.enable=true may help to solve this issue.\n",
      "\n",
      "If you are still having trouble, try setting the log.connection.close client configuration property to false to silence all spontaneous disconnect logs.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/confluentinc/librdkafka/wiki/FAQ\n",
    "prompt=\"\"\"\n",
    "  system: principal data engineer tutoring junior data engineers\n",
    "  knowledge: \\\"\\\"\\\"\n",
    "  Q: Why am I seeing Receive failed: Disconnected?\n",
    "  A: If the remote peer, typically the broker (but could also be an active TCP gateway of some kind), closes the connection you'll see a log message like this:\n",
    "\n",
    "%3|1500588440.537|FAIL|rdkafka#producer-1| 10.255.84.150:9092/1: Receive failed: Disconnected\n",
    "\n",
    "There are a number of possible reasons, in order of how common they are:\n",
    "\n",
    "Broker's idle connection reaper closes the connection due to inactivity. This is controlled by the broker configuration property connections.max.idle.ms and defaults to 10 minutes. This is by far the most common reason for spontaneous disconnects.\n",
    "The client sent an unsupported protocol request; see Broker version compatibility. This is considered a configuration error on the client. The broker should log an exception explaining why the connection was closed, see the broker logs.\n",
    "The client sent a malformed protocol request; this is an indication of a bug in the client. The broker should log an exception explaining why the connection was closed, see the broker logs.\n",
    "The broker is in an invalid state. The broker should log an exception explaining why the connection was closed, see the broker logs.\n",
    "TCP gateway/load-balancer/firewall session timeout. Try enabling TCP keep-alives on the client by setting socket.keepalive.enable=true.\n",
    "Since a TCP close can't signal why the remote peer closed the connection there is no way for the client to know what went wrong. If the disconnect logs are getting annoying and the admin deems they are caused by the idle connection reaper, the log.connection.close client configuration property can be set to false to silence all spontaneous disconnect logs.\n",
    "  Q: Why am I not seeing any messages?\n",
    "  A: If there are no stored offsets for a partition (and group in case of the KafkaConsumer) the consumer will default its starting offset to the topic configuration setting auto.offset.reset which defaults to latest - that is, it will start consuming at the current end of a partition.\n",
    "\n",
    "If you are using the KafkaConsumer you probably do not have a per-topic configuration object but should use the default topic config, see default_topic_conf.\n",
    "  \\\"\\\"\\\"\n",
    "  prompt: I am getting Receive failed: Disconnected error, I doubt if I sent an unsupported protocol request. tell me how to solve the issue\n",
    "  \"\"\"\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 PAL - Code as Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are developing a simple application that's able to reason about the question being asked through code. \n",
    "\n",
    "Specifically, the application takes in some data and answers a question about the data input. The prompt includes a few exemplars which are adopted from [here](https://github.com/reasoning-machines/pal/blob/main/pal/prompt/penguin_prompt.py).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " knowledge: Q: write a basic producer utilising the provided Kafka class KafkaProducer, which will publish to testTopic package main;  import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord;  import java.util.Properties;  public class KafkaProducerExample {      private static final String TOPIC = \"test-topic\";      public static void main(String[] args) {         Properties settings = setUpProperties();         KafkaProducer<String, String> producer =  new KafkaProducer<>(settings);          Runtime.getRuntime().addShutdownHook(new Thread(() -> {             System.out.println(\"...Stopping Basic Producer...\");             producer.close();         }));          publishData(producer);     }      private static Properties setUpProperties() {         Properties settings = new Properties();         settings.put(ProducerConfig.CLIENT_ID_CONFIG, \"basic-producer\");         settings.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");         settings.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\");         settings.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\");         return settings;     }      private static void publishData(KafkaProducer producer) {         for (int index = 0; index < 5; index++) {             final String key = \"key-\" + index;             final String value = \"value-\" + index;             final ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC, key, value);             producer.send(record);         }     } }    Q: Set up Consumer in Java for Kafka test topic package main;  import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.common.serialization.StringDeserializer;  import java.util.Properties;  import static java.time.Duration.ofMillis; import static java.util.Collections.singletonList; import static org.apache.kafka.clients.consumer.ConsumerConfig.*;  public class KafkaConsumerExample {     private static final String TOPIC = \"test-topic\";      public static void main(String[] args) {         Properties settings = setUpProperties();         KafkaConsumer<String, String> consumer = new KafkaConsumer<>(settings);          Runtime.getRuntime().addShutdownHook(new Thread(() -> {             System.out.println(\"...Stopping Basic Consumer...\");             consumer.close();         }));          consumeData(consumer);     }      private static Properties setUpProperties() {         Properties settings = new Properties();         settings.put(GROUP_ID_CONFIG, \"basic-consumer\");         settings.put(BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");         settings.put(ENABLE_AUTO_COMMIT_CONFIG, \"true\");         settings.put(AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\");         settings.put(AUTO_OFFSET_RESET_CONFIG, \"earliest\");         settings.put(KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);         settings.put(VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);         return settings;     }      private static void consumeData(KafkaConsumer consumer) {         consumer.subscribe(singletonList(TOPIC));         while (true) {             ConsumerRecords<String, String> records = consumer.poll(ofMillis(100));             for (ConsumerRecord<String, String> record : records) {                 System.out.printf(\"message offset = %d, key = %s, value = %s\\n\",                         record.offset(), record.key(), record.value());             }         }     } }    prompt: write me a mini application including producer and consumer for Kafka in Java\n",
      "\n",
      "\n",
      "Sure, here's a mini application that includes both a Kafka producer and consumer in Java:\n",
      "\n",
      "package main;\n",
      "\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecord;\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecords;\n",
      "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
      "import org.apache.kafka.clients.producer.KafkaProducer;\n",
      "import org.apache.kafka.clients.producer.ProducerConfig;\n",
      "import org.apache.kafka.clients.producer.ProducerRecord;\n",
      "import org.apache.kafka.common.serialization.StringDeserializer;\n",
      "import org.apache.kafka.common.serialization.StringSerializer;\n",
      "\n",
      "import java.util.Collections;\n",
      "import java.util.Properties;\n",
      "\n",
      "import static java.time.Duration.ofMillis;\n",
      "\n",
      "public class KafkaExample {\n",
      "\n",
      "    private static final String TOPIC = \"test-topic\";\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "\n",
      "        // set up producer\n",
      "        Properties producerProps = new Properties();\n",
      "        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"basic-producer\");\n",
      "        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n",
      "        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n",
      "        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n",
      "\n",
      "        KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);\n",
      "\n",
      "        // set up consumer\n",
      "        Properties consumerProps = new Properties();\n",
      "        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, \"basic-consumer\");\n",
      "        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n",
      "        consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\");\n",
      "        consumerProps.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\");\n",
      "        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n",
      "        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
      "        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
      "\n",
      "        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);\n",
      "\n",
      "        // subscribe to topic\n",
      "        consumer.subscribe(Collections.singleton(TOPIC));\n",
      "\n",
      "        // produce messages\n",
      "        for (int i = 0; i < 5; i++) {\n",
      "            ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC, \"key-\" + i, \"value-\" + i);\n",
      "            producer.send(record);\n",
      "        }\n",
      "\n",
      "        // consume messages\n",
      "        while (true) {\n",
      "            ConsumerRecords<String, String> records = consumer.poll(ofMillis(100));\n",
      "\n",
      "            for (ConsumerRecord<String, String> record : records) {\n",
      "                System.out.printf(\"message offset = %d, key = %s, value = %s\\n\",\n",
      "                        record.offset(), record.key(), record.value());\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "This code sets up a Kafka producer and consumer, subscribes to the \"test-topic\" topic, and then produces and consumes a few messages. The producer sends 5 messages to the topic with keys and values like \"key-1\" and \"value-1\", and the consumer polls the topic for new messages and prints out their offsets, keys, and values.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"user\": \"\"\"\n",
    "  knowledge: Q: write a basic producer utilising the provided Kafka class KafkaProducer, which will publish to testTopic\n",
    "package main;\n",
    "\n",
    "import org.apache.kafka.clients.producer.KafkaProducer;\n",
    "import org.apache.kafka.clients.producer.ProducerConfig;\n",
    "import org.apache.kafka.clients.producer.ProducerRecord;\n",
    "\n",
    "import java.util.Properties;\n",
    "\n",
    "public class KafkaProducerExample {\n",
    "\n",
    "    private static final String TOPIC = \"test-topic\";\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Properties settings = setUpProperties();\n",
    "        KafkaProducer<String, String> producer =  new KafkaProducer<>(settings);\n",
    "\n",
    "        Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n",
    "            System.out.println(\"...Stopping Basic Producer...\");\n",
    "            producer.close();\n",
    "        }));\n",
    "\n",
    "        publishData(producer);\n",
    "    }\n",
    "\n",
    "    private static Properties setUpProperties() {\n",
    "        Properties settings = new Properties();\n",
    "        settings.put(ProducerConfig.CLIENT_ID_CONFIG, \"basic-producer\");\n",
    "        settings.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n",
    "        settings.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "        settings.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "        return settings;\n",
    "    }\n",
    "\n",
    "    private static void publishData(KafkaProducer producer) {\n",
    "        for (int index = 0; index < 5; index++) {\n",
    "            final String key = \"key-\" + index;\n",
    "            final String value = \"value-\" + index;\n",
    "            final ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC, key, value);\n",
    "            producer.send(record);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "  \n",
    "Q: Set up Consumer in Java for Kafka test topic\n",
    "package main;\n",
    "\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecords;\n",
    "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
    "import org.apache.kafka.common.serialization.StringDeserializer;\n",
    "\n",
    "import java.util.Properties;\n",
    "\n",
    "import static java.time.Duration.ofMillis;\n",
    "import static java.util.Collections.singletonList;\n",
    "import static org.apache.kafka.clients.consumer.ConsumerConfig.*;\n",
    "\n",
    "public class KafkaConsumerExample {\n",
    "    private static final String TOPIC = \"test-topic\";\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Properties settings = setUpProperties();\n",
    "        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(settings);\n",
    "\n",
    "        Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n",
    "            System.out.println(\"...Stopping Basic Consumer...\");\n",
    "            consumer.close();\n",
    "        }));\n",
    "\n",
    "        consumeData(consumer);\n",
    "    }\n",
    "\n",
    "    private static Properties setUpProperties() {\n",
    "        Properties settings = new Properties();\n",
    "        settings.put(GROUP_ID_CONFIG, \"basic-consumer\");\n",
    "        settings.put(BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n",
    "        settings.put(ENABLE_AUTO_COMMIT_CONFIG, \"true\");\n",
    "        settings.put(AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\");\n",
    "        settings.put(AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n",
    "        settings.put(KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "        settings.put(VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "        return settings;\n",
    "    }\n",
    "\n",
    "    private static void consumeData(KafkaConsumer consumer) {\n",
    "        consumer.subscribe(singletonList(TOPIC));\n",
    "        while (true) {\n",
    "            ConsumerRecords<String, String> records = consumer.poll(ofMillis(100));\n",
    "            for (ConsumerRecord<String, String> record : records) {\n",
    "                System.out.printf(\"message offset = %d, key = %s, value = %s\\n\",\n",
    "                        record.offset(), record.key(), record.value());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "  prompt: write me a mini application including producer and consumer for Kafka in Java\n",
    "  \n",
    "  \"\"\",\n",
    "  \"lang\": \"english\"\n",
    "}\n",
    "\n",
    "get_response_from_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the prompt and question. We can send it to the model. It should output the steps, in code, needed to get the solution to the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the correct answer! Vincent is the oldest penguin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Try a different question and see what's the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tools and Applications\n",
    "\n",
    "Objective:\n",
    "\n",
    "- Demonstrate how to use LangChain to demonstrate simple applications using prompting techniques and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LLMs & External Tools\n",
    "\n",
    "Example adopted from the [LangChain documentation](https://langchain.readthedocs.io/en/latest/modules/agents/getting_started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] =openai.api_key\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what employers are looking for in Apache Kafka\n",
      "Action: Search\n",
      "Action Input: \"what employers are looking for in Apache Kafka\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBrowse 1-20 of 2580 available Apache Kafka jobs on Dice.com. Apply to Java Developer, Data Engineer, Full Stack Developer and more.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out more specific details\n",
      "Action: Search\n",
      "Action Input: \"what skills employers are looking for in Apache Kafka\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTop Tech Skills of 2020 Include Swift, Kafka · Hadoop Linux Cloudera Range Atlas Engineer · Hadoop Linux Engineer · DevSecOps Cloud Engineer · Sr.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Employers are looking for skills such as Swift, Kafka, Hadoop Linux Cloudera Range Atlas Engineer, Hadoop Linux Engineer, DevSecOps Cloud Engineer, and Sr.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Employers are looking for skills such as Swift, Kafka, Hadoop Linux Cloudera Range Atlas Engineer, Hadoop Linux Engineer, DevSecOps Cloud Engineer, and Sr.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the agent\n",
    "agent.run(\"what employers are looking for in Apache Kafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data we want to use as source to augment generation.\n",
    "\n",
    "Code example adopted from [LangChain Documentation](https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/qa_with_sources.html). We are only using the examples for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Justice Breyer\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a question with a custom prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "Respond in Spanish.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER IN SPANISH:\"\"\"\n",
    "\n",
    "# create a prompt template\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# query \n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
    "query = \"What did the president say about Justice Breyer?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Try using a different dataset from the internet and try different prompt, including all the techniques you learned in the lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b052b85184348e6013a65b4e7f9532c753d0b79191b90d8994bc68bec0cc322a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
